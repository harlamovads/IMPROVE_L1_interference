{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dav6465-yCdh",
        "outputId": "113e545b-c7c0-436c-a373-2f520752b941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/adv_gan.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "vrGkn95Y8c2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfMfWvoTxxEO"
      },
      "outputs": [],
      "source": [
        "!pip install -r TextGAN-PyTorch/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://kheafield.com/code/kenlm.tar.gz\n",
        "!tar -xzvf kenlm.tar.gz"
      ],
      "metadata": {
        "id": "OU-J6Gu_78Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd kenlm && mkdir -p build && cd build && cmake .. && make -j 4 && pip install https://github.com/kpu/kenlm/archive/master.zip"
      ],
      "metadata": {
        "id": "IXxTXU3H8dTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "aogvUjmsHgEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "aGrCzq9YHwaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TextGAN-PyTorch"
      ],
      "metadata": {
        "id": "gf1SvJfiJmC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_checkpoint  = torch.load(\"/content/gen_ADV_00220.pt\", map_location=\"cpu\")"
      ],
      "metadata": {
        "id": "31st-16-JeAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.CatGAN_G import CatGAN_G"
      ],
      "metadata": {
        "id": "dsPAU4q4JiZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "5rVlapK4Jutj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatGAN_G(5, 1, 2, 256, 32, 32, 3104, 31, 0, False).to(device)"
      ],
      "metadata": {
        "id": "yvTSOlgsJyt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in gen_checkpoint.items():\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "j4HvFmInLI2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e81d5eb-7eae-4dd9-85d1-aca054efea5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('temperature', tensor([1.]))\n",
            "('cat_mat', tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1.]]))\n",
            "('pos_encoder.pe', tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00]],\n",
            "\n",
            "        [[ 8.4147e-01,  5.4030e-01,  5.3317e-01,  ...,  1.0000e+00,\n",
            "           1.7783e-04,  1.0000e+00]],\n",
            "\n",
            "        [[ 9.0930e-01, -4.1615e-01,  9.0213e-01,  ...,  1.0000e+00,\n",
            "           3.5566e-04,  1.0000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 9.5625e-01, -2.9254e-01,  9.9089e-01,  ..., -9.3940e-03,\n",
            "           7.7619e-01,  6.3049e-01]],\n",
            "\n",
            "        [[ 2.7050e-01, -9.6272e-01,  9.1005e-01,  ..., -9.7101e-03,\n",
            "           7.7631e-01,  6.3036e-01]],\n",
            "\n",
            "        [[-6.6395e-01, -7.4778e-01,  5.4898e-01,  ..., -1.0026e-02,\n",
            "           7.7642e-01,  6.3022e-01]]]))\n",
            "('transformer_encoder.layers.0.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.0.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.0.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.0.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.0.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.0.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.0.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.0.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.0.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.0.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.0.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.0.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.1.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.1.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.1.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.1.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.1.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.1.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.1.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.1.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.1.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.1.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.1.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.1.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.2.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.2.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.2.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.2.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.2.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.2.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.2.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.2.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.2.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.2.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.2.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.2.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.3.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.3.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.3.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.3.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.3.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.3.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.3.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.3.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.3.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.3.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.3.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.3.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.4.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.4.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.4.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.4.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.4.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.4.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.4.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.4.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.4.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.4.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.4.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.4.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.5.self_attn.in_proj_weight', tensor([[ 0.2047,  0.1622, -0.0984,  ...,  0.0118,  0.1964, -0.1133],\n",
            "        [-0.1511, -0.2097,  0.0688,  ..., -0.1929,  0.1716, -0.1062],\n",
            "        [ 0.1036,  0.0448, -0.0863,  ...,  0.0545, -0.0405, -0.1774],\n",
            "        ...,\n",
            "        [ 0.1444, -0.1523,  0.0690,  ...,  0.1644,  0.1899,  0.1111],\n",
            "        [ 0.1458,  0.0635, -0.1939,  ..., -0.1532, -0.0442,  0.0948],\n",
            "        [-0.0606,  0.0871,  0.0855,  ..., -0.1100, -0.0323, -0.2073]]))\n",
            "('transformer_encoder.layers.5.self_attn.in_proj_bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.5.self_attn.out_proj.weight', tensor([[ 0.1338,  0.0131,  0.0731,  ...,  0.1138,  0.0081,  0.1726],\n",
            "        [ 0.1296, -0.1069, -0.0699,  ..., -0.1236, -0.0328, -0.1240],\n",
            "        [-0.0445,  0.0362, -0.1760,  ...,  0.1668, -0.0398,  0.1291],\n",
            "        ...,\n",
            "        [ 0.1530,  0.0965, -0.0550,  ..., -0.0935,  0.1266,  0.0171],\n",
            "        [ 0.0585, -0.0693, -0.0243,  ..., -0.1369,  0.1141,  0.1563],\n",
            "        [ 0.0343,  0.1566, -0.0902,  ..., -0.0226,  0.0128, -0.0482]]))\n",
            "('transformer_encoder.layers.5.self_attn.out_proj.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.5.linear1.weight', tensor([[-0.0972, -0.0984,  0.0459,  ..., -0.1238, -0.0850,  0.0153],\n",
            "        [ 0.1436, -0.1082,  0.1358,  ..., -0.1485,  0.1655, -0.1144],\n",
            "        [ 0.0580, -0.1280,  0.0713,  ...,  0.1370, -0.1607,  0.0163],\n",
            "        ...,\n",
            "        [ 0.1559, -0.0049, -0.0994,  ..., -0.0410, -0.0167,  0.1324],\n",
            "        [-0.0502, -0.0319,  0.1620,  ..., -0.0748,  0.0679, -0.0933],\n",
            "        [ 0.1654,  0.1083, -0.0924,  ...,  0.1662, -0.0043, -0.0885]]))\n",
            "('transformer_encoder.layers.5.linear1.bias', tensor([ 0.1285,  0.0393,  0.1462, -0.0684, -0.1007,  0.1264, -0.0911, -0.0097,\n",
            "         0.0936,  0.0319, -0.0056, -0.0284,  0.1389, -0.1462,  0.0216,  0.0856,\n",
            "         0.1750, -0.1357,  0.1704, -0.0522,  0.1510, -0.0116, -0.0103,  0.1732,\n",
            "        -0.0121, -0.0772, -0.1203,  0.1150, -0.0503,  0.1035,  0.1619, -0.1533]))\n",
            "('transformer_encoder.layers.5.linear2.weight', tensor([[-0.1102,  0.1597, -0.1720,  ..., -0.0877,  0.0642, -0.0775],\n",
            "        [-0.1107, -0.1460,  0.1649,  ...,  0.1110, -0.0898,  0.0383],\n",
            "        [ 0.0563,  0.0096,  0.0321,  ...,  0.0854,  0.0095, -0.1261],\n",
            "        ...,\n",
            "        [-0.1028,  0.0210,  0.0520,  ..., -0.1439,  0.0502,  0.1635],\n",
            "        [ 0.1602,  0.1205,  0.1072,  ...,  0.0452,  0.1210, -0.0315],\n",
            "        [-0.0303,  0.0829, -0.1764,  ...,  0.0770,  0.1233,  0.1121]]))\n",
            "('transformer_encoder.layers.5.linear2.bias', tensor([-0.0791, -0.1159,  0.0627,  0.0983,  0.1653, -0.1066, -0.1097,  0.1292,\n",
            "         0.0750,  0.0723,  0.0798, -0.1333, -0.1051, -0.0573, -0.1650,  0.0399,\n",
            "         0.1626,  0.0069, -0.1741,  0.1669, -0.1255, -0.0608, -0.1668,  0.0827,\n",
            "        -0.0651,  0.1221,  0.1723, -0.0849, -0.1643,  0.1313,  0.0571, -0.1020]))\n",
            "('transformer_encoder.layers.5.norm1.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.5.norm1.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('transformer_encoder.layers.5.norm2.weight', tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))\n",
            "('transformer_encoder.layers.5.norm2.bias', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.]))\n",
            "('embeddings.weight', tensor([[ 0.0427,  0.0968, -0.0727,  ...,  0.0926, -0.0185, -0.0949],\n",
            "        [ 0.0961, -0.0407,  0.0128,  ..., -0.0715, -0.0116,  0.0687],\n",
            "        [-0.0223,  0.1257,  0.0240,  ...,  0.1106, -0.0226, -0.0673],\n",
            "        ...,\n",
            "        [-0.1211, -0.0547,  0.0796,  ...,  0.0129,  0.0356,  0.0928],\n",
            "        [ 0.0705, -0.0268,  0.0647,  ..., -0.1174, -0.0185,  0.0651],\n",
            "        [ 0.1782,  0.0892,  0.0254,  ...,  0.0307,  0.0467, -0.0888]]))\n",
            "('linear.weight', tensor([[-0.0772,  0.0815, -0.0376,  ...,  0.0478,  0.0636,  0.0797],\n",
            "        [-0.0280,  0.0437,  0.0630,  ...,  0.0912, -0.0497,  0.0145],\n",
            "        [ 0.0176, -0.0592,  0.0212,  ..., -0.0291,  0.0689,  0.0782],\n",
            "        ...,\n",
            "        [-0.0722, -0.0426,  0.0345,  ...,  0.0196,  0.0647, -0.0996],\n",
            "        [ 0.0703,  0.0629,  0.0887,  ..., -0.0995, -0.0945, -0.0894],\n",
            "        [ 0.0111,  0.0342,  0.0427,  ...,  0.0378,  0.0175,  0.0283]]))\n",
            "('linear.bias', tensor([0., 0., 0.,  ..., 0., 0., 0.]))\n",
            "('lstm.forget_bias', tensor(1.0122))\n",
            "('lstm.input_bias', tensor(0.1540))\n",
            "('lstm.qkv_projector.weight', tensor([[-0.0121,  0.0147, -0.0241,  ..., -0.0487, -0.0507,  0.0347],\n",
            "        [-0.0313,  0.0296, -0.0235,  ...,  0.0435,  0.0114, -0.0122],\n",
            "        [-0.0519, -0.0300,  0.0259,  ...,  0.0040, -0.0158, -0.0246],\n",
            "        ...,\n",
            "        [-0.0187,  0.0056, -0.0041,  ...,  0.0185, -0.0036,  0.0191],\n",
            "        [-0.0039,  0.0182, -0.0255,  ..., -0.0291,  0.0055,  0.0549],\n",
            "        [-0.0280, -0.0357, -0.0258,  ..., -0.0476,  0.0540,  0.0028]]))\n",
            "('lstm.qkv_projector.bias', tensor([-0.0004,  0.0018, -0.0207,  ..., -0.0249, -0.0061,  0.0128]))\n",
            "('lstm.qkv_layernorm.weight', tensor([[1.0288, 1.0324, 1.0213,  ..., 1.0655, 0.9996, 1.0170],\n",
            "        [1.0079, 1.0651, 1.0190,  ..., 1.0665, 1.0662, 1.0464]]))\n",
            "('lstm.qkv_layernorm.bias', tensor([[ 0.0200,  0.0304,  0.0033,  ..., -0.0324,  0.0107, -0.0071],\n",
            "        [-0.0220, -0.0797, -0.0838,  ...,  0.0177,  0.0024,  0.0016]]))\n",
            "('lstm.attention_mlp.0.weight', tensor([[-0.0243,  0.0497,  0.0450,  ..., -0.0030,  0.0435,  0.0329],\n",
            "        [ 0.0094, -0.0292,  0.0446,  ...,  0.0207,  0.0174,  0.0099],\n",
            "        [-0.0094,  0.0229, -0.0467,  ...,  0.0050,  0.0062,  0.0346],\n",
            "        ...,\n",
            "        [ 0.0238, -0.0283,  0.0456,  ...,  0.0130,  0.0318,  0.0005],\n",
            "        [-0.0076,  0.0475, -0.0503,  ..., -0.0062, -0.0026, -0.0185],\n",
            "        [-0.0236,  0.0101, -0.0267,  ...,  0.0124, -0.0332,  0.0287]]))\n",
            "('lstm.attention_mlp.0.bias', tensor([ 0.0154, -0.0096, -0.0397, -0.0137,  0.0184, -0.0253,  0.0440,  0.0496,\n",
            "         0.0313,  0.0515, -0.0449, -0.0113,  0.0507,  0.0430, -0.0399,  0.0283,\n",
            "        -0.0513,  0.0134,  0.0248,  0.0573,  0.0023,  0.0212,  0.0068, -0.0108,\n",
            "        -0.0182,  0.0187,  0.0313,  0.0281,  0.0286, -0.0331,  0.0402,  0.0413,\n",
            "         0.0382, -0.0249,  0.0504, -0.0379, -0.0234, -0.0348,  0.0129,  0.0222,\n",
            "         0.0035,  0.0365,  0.0088, -0.0160,  0.0218, -0.0203,  0.0338,  0.0241,\n",
            "        -0.0326,  0.0054, -0.0435, -0.0320,  0.0471,  0.0146, -0.0152, -0.0260,\n",
            "         0.0310,  0.0057,  0.0373,  0.0228,  0.0551,  0.0369, -0.0132,  0.0483,\n",
            "         0.0093, -0.0374, -0.0092, -0.0093,  0.0457,  0.0102, -0.0209,  0.0778,\n",
            "        -0.0125, -0.0236, -0.0177,  0.0133,  0.0221,  0.0543,  0.0170, -0.0066,\n",
            "         0.0041,  0.0189,  0.0087, -0.0296, -0.0281,  0.0081,  0.0299, -0.0148,\n",
            "         0.0537,  0.0403,  0.0436,  0.0352, -0.0028,  0.0003,  0.0249,  0.0039,\n",
            "        -0.0043,  0.0047,  0.0296,  0.0237,  0.0125, -0.0288, -0.0194,  0.0527,\n",
            "         0.0616,  0.0265,  0.0203, -0.0283,  0.0340,  0.0432,  0.0502, -0.0358,\n",
            "        -0.0131, -0.0026, -0.0380, -0.0228,  0.0365,  0.0248, -0.0091,  0.0103,\n",
            "         0.0411,  0.0457,  0.0134,  0.0147, -0.0166,  0.0111, -0.0048, -0.0374,\n",
            "        -0.0041,  0.0106, -0.0379,  0.0224,  0.0512,  0.0038, -0.0150,  0.0008,\n",
            "         0.0382,  0.0285,  0.0311, -0.0143, -0.0162, -0.0316, -0.0270,  0.0261,\n",
            "        -0.0193,  0.0639,  0.0148,  0.0307,  0.0194,  0.0471,  0.0019, -0.0157,\n",
            "         0.0160,  0.0400,  0.0372,  0.0489,  0.0225,  0.0283,  0.0249, -0.0120,\n",
            "        -0.0329, -0.0094,  0.0389,  0.0469, -0.0104,  0.0442, -0.0341,  0.0234,\n",
            "         0.0085,  0.0205, -0.0069, -0.0153, -0.0371,  0.0070,  0.0273,  0.0121,\n",
            "        -0.0086, -0.0127, -0.0172,  0.0558,  0.0140,  0.0483,  0.0389,  0.0228,\n",
            "         0.0175,  0.0229, -0.0142,  0.0555, -0.0391,  0.0258, -0.0170,  0.0140,\n",
            "         0.0342,  0.0477,  0.0054, -0.0121,  0.0462,  0.0533, -0.0007, -0.0012,\n",
            "         0.0300,  0.0484,  0.0526, -0.0191, -0.0024, -0.0146,  0.0019, -0.0233,\n",
            "         0.0408,  0.0321,  0.0056,  0.0180,  0.0010, -0.0248, -0.0078,  0.0208,\n",
            "        -0.0442,  0.0019,  0.0171,  0.0079, -0.0199, -0.0074,  0.0094,  0.0204,\n",
            "         0.0288,  0.0473,  0.0431,  0.0094, -0.0226, -0.0008,  0.0288, -0.0300,\n",
            "        -0.0293, -0.0153, -0.0237,  0.0422,  0.0267,  0.0348,  0.0273,  0.0202,\n",
            "        -0.0198, -0.0317,  0.0331,  0.0032,  0.0120, -0.0062,  0.0438, -0.0315,\n",
            "         0.0545,  0.0035, -0.0023, -0.0185,  0.0268,  0.0409, -0.0103, -0.0331,\n",
            "         0.0144,  0.0231, -0.0306,  0.0189,  0.0217, -0.0212, -0.0063, -0.0371,\n",
            "         0.0297, -0.0352, -0.0040, -0.0335,  0.0205,  0.0179,  0.0233,  0.0200,\n",
            "        -0.0442, -0.0045,  0.0550, -0.0072, -0.0215,  0.0124,  0.0567,  0.0212,\n",
            "         0.0178, -0.0407,  0.0023,  0.0244,  0.0301,  0.0047,  0.0149,  0.0220,\n",
            "        -0.0167, -0.0209,  0.0450,  0.0399, -0.0007,  0.0441, -0.0058,  0.0035,\n",
            "        -0.0361, -0.0376, -0.0257,  0.0129,  0.0246, -0.0098,  0.0511,  0.0540,\n",
            "        -0.0307,  0.0497,  0.0505,  0.0368, -0.0107, -0.0160,  0.0120,  0.0371,\n",
            "        -0.0277,  0.0516,  0.0073, -0.0321, -0.0057,  0.0034, -0.0237,  0.0260,\n",
            "         0.0274,  0.0225, -0.0194, -0.0220,  0.0237,  0.0163,  0.0064,  0.0125,\n",
            "         0.0073,  0.0423,  0.0101,  0.0121,  0.0146,  0.0383,  0.0250,  0.0077,\n",
            "         0.0089, -0.0090,  0.0278,  0.0471,  0.0150,  0.0333, -0.0082, -0.0136,\n",
            "        -0.0184,  0.0233,  0.0578, -0.0367, -0.0079,  0.0267, -0.0043,  0.0010,\n",
            "        -0.0068,  0.0390, -0.0037, -0.0251, -0.0292,  0.0319, -0.0021,  0.0374,\n",
            "         0.0490,  0.0510,  0.0263,  0.0310, -0.0241, -0.0245,  0.0098,  0.0439,\n",
            "        -0.0206,  0.0321,  0.0510, -0.0317, -0.0123,  0.0092,  0.0346,  0.0363,\n",
            "        -0.0006,  0.0498, -0.0201, -0.0096,  0.0369,  0.0529,  0.0316, -0.0252,\n",
            "        -0.0339, -0.0176, -0.0271,  0.0136,  0.0289,  0.0110,  0.0070, -0.0025,\n",
            "         0.0238,  0.0384,  0.0046,  0.0270,  0.0044,  0.0445,  0.0430,  0.0391,\n",
            "         0.0267, -0.0393,  0.0275, -0.0357,  0.0385, -0.0034,  0.0441,  0.0474,\n",
            "        -0.0080, -0.0236,  0.0486, -0.0187, -0.0017, -0.0003, -0.0397, -0.0031,\n",
            "         0.0048,  0.0511,  0.0275, -0.0171, -0.0491, -0.0131,  0.0286,  0.0093,\n",
            "        -0.0359,  0.0265,  0.0189,  0.0177, -0.0031,  0.0436,  0.0031,  0.0088,\n",
            "        -0.0315,  0.0168, -0.0072, -0.0280,  0.0427,  0.0431, -0.0134, -0.0474,\n",
            "         0.0089,  0.0453, -0.0239,  0.0101,  0.0184,  0.0436, -0.0158, -0.0320,\n",
            "        -0.0107,  0.0045,  0.0050,  0.0145, -0.0274,  0.0120, -0.0120,  0.0447,\n",
            "         0.0151,  0.0383, -0.0144, -0.0389, -0.0154, -0.0303, -0.0352, -0.0200,\n",
            "         0.0538,  0.0441,  0.0223,  0.0249, -0.0231,  0.0407,  0.0133,  0.0650,\n",
            "        -0.0264,  0.0268,  0.0219, -0.0150, -0.0401,  0.0320, -0.0079,  0.0514,\n",
            "        -0.0065,  0.0073,  0.0354,  0.0211, -0.0347,  0.0208,  0.0460, -0.0259,\n",
            "         0.0009,  0.0216, -0.0232,  0.0473, -0.0215,  0.0462, -0.0167, -0.0281,\n",
            "         0.0474, -0.0323, -0.0142, -0.0069, -0.0238,  0.0491,  0.0329,  0.0395,\n",
            "         0.0166, -0.0105,  0.0521,  0.0179,  0.0529,  0.0068,  0.0060, -0.0148]))\n",
            "('lstm.attention_mlp.1.weight', tensor([[-0.0243,  0.0497,  0.0450,  ..., -0.0030,  0.0435,  0.0329],\n",
            "        [ 0.0094, -0.0292,  0.0446,  ...,  0.0207,  0.0174,  0.0099],\n",
            "        [-0.0094,  0.0229, -0.0467,  ...,  0.0050,  0.0062,  0.0346],\n",
            "        ...,\n",
            "        [ 0.0238, -0.0283,  0.0456,  ...,  0.0130,  0.0318,  0.0005],\n",
            "        [-0.0076,  0.0475, -0.0503,  ..., -0.0062, -0.0026, -0.0185],\n",
            "        [-0.0236,  0.0101, -0.0267,  ...,  0.0124, -0.0332,  0.0287]]))\n",
            "('lstm.attention_mlp.1.bias', tensor([ 0.0154, -0.0096, -0.0397, -0.0137,  0.0184, -0.0253,  0.0440,  0.0496,\n",
            "         0.0313,  0.0515, -0.0449, -0.0113,  0.0507,  0.0430, -0.0399,  0.0283,\n",
            "        -0.0513,  0.0134,  0.0248,  0.0573,  0.0023,  0.0212,  0.0068, -0.0108,\n",
            "        -0.0182,  0.0187,  0.0313,  0.0281,  0.0286, -0.0331,  0.0402,  0.0413,\n",
            "         0.0382, -0.0249,  0.0504, -0.0379, -0.0234, -0.0348,  0.0129,  0.0222,\n",
            "         0.0035,  0.0365,  0.0088, -0.0160,  0.0218, -0.0203,  0.0338,  0.0241,\n",
            "        -0.0326,  0.0054, -0.0435, -0.0320,  0.0471,  0.0146, -0.0152, -0.0260,\n",
            "         0.0310,  0.0057,  0.0373,  0.0228,  0.0551,  0.0369, -0.0132,  0.0483,\n",
            "         0.0093, -0.0374, -0.0092, -0.0093,  0.0457,  0.0102, -0.0209,  0.0778,\n",
            "        -0.0125, -0.0236, -0.0177,  0.0133,  0.0221,  0.0543,  0.0170, -0.0066,\n",
            "         0.0041,  0.0189,  0.0087, -0.0296, -0.0281,  0.0081,  0.0299, -0.0148,\n",
            "         0.0537,  0.0403,  0.0436,  0.0352, -0.0028,  0.0003,  0.0249,  0.0039,\n",
            "        -0.0043,  0.0047,  0.0296,  0.0237,  0.0125, -0.0288, -0.0194,  0.0527,\n",
            "         0.0616,  0.0265,  0.0203, -0.0283,  0.0340,  0.0432,  0.0502, -0.0358,\n",
            "        -0.0131, -0.0026, -0.0380, -0.0228,  0.0365,  0.0248, -0.0091,  0.0103,\n",
            "         0.0411,  0.0457,  0.0134,  0.0147, -0.0166,  0.0111, -0.0048, -0.0374,\n",
            "        -0.0041,  0.0106, -0.0379,  0.0224,  0.0512,  0.0038, -0.0150,  0.0008,\n",
            "         0.0382,  0.0285,  0.0311, -0.0143, -0.0162, -0.0316, -0.0270,  0.0261,\n",
            "        -0.0193,  0.0639,  0.0148,  0.0307,  0.0194,  0.0471,  0.0019, -0.0157,\n",
            "         0.0160,  0.0400,  0.0372,  0.0489,  0.0225,  0.0283,  0.0249, -0.0120,\n",
            "        -0.0329, -0.0094,  0.0389,  0.0469, -0.0104,  0.0442, -0.0341,  0.0234,\n",
            "         0.0085,  0.0205, -0.0069, -0.0153, -0.0371,  0.0070,  0.0273,  0.0121,\n",
            "        -0.0086, -0.0127, -0.0172,  0.0558,  0.0140,  0.0483,  0.0389,  0.0228,\n",
            "         0.0175,  0.0229, -0.0142,  0.0555, -0.0391,  0.0258, -0.0170,  0.0140,\n",
            "         0.0342,  0.0477,  0.0054, -0.0121,  0.0462,  0.0533, -0.0007, -0.0012,\n",
            "         0.0300,  0.0484,  0.0526, -0.0191, -0.0024, -0.0146,  0.0019, -0.0233,\n",
            "         0.0408,  0.0321,  0.0056,  0.0180,  0.0010, -0.0248, -0.0078,  0.0208,\n",
            "        -0.0442,  0.0019,  0.0171,  0.0079, -0.0199, -0.0074,  0.0094,  0.0204,\n",
            "         0.0288,  0.0473,  0.0431,  0.0094, -0.0226, -0.0008,  0.0288, -0.0300,\n",
            "        -0.0293, -0.0153, -0.0237,  0.0422,  0.0267,  0.0348,  0.0273,  0.0202,\n",
            "        -0.0198, -0.0317,  0.0331,  0.0032,  0.0120, -0.0062,  0.0438, -0.0315,\n",
            "         0.0545,  0.0035, -0.0023, -0.0185,  0.0268,  0.0409, -0.0103, -0.0331,\n",
            "         0.0144,  0.0231, -0.0306,  0.0189,  0.0217, -0.0212, -0.0063, -0.0371,\n",
            "         0.0297, -0.0352, -0.0040, -0.0335,  0.0205,  0.0179,  0.0233,  0.0200,\n",
            "        -0.0442, -0.0045,  0.0550, -0.0072, -0.0215,  0.0124,  0.0567,  0.0212,\n",
            "         0.0178, -0.0407,  0.0023,  0.0244,  0.0301,  0.0047,  0.0149,  0.0220,\n",
            "        -0.0167, -0.0209,  0.0450,  0.0399, -0.0007,  0.0441, -0.0058,  0.0035,\n",
            "        -0.0361, -0.0376, -0.0257,  0.0129,  0.0246, -0.0098,  0.0511,  0.0540,\n",
            "        -0.0307,  0.0497,  0.0505,  0.0368, -0.0107, -0.0160,  0.0120,  0.0371,\n",
            "        -0.0277,  0.0516,  0.0073, -0.0321, -0.0057,  0.0034, -0.0237,  0.0260,\n",
            "         0.0274,  0.0225, -0.0194, -0.0220,  0.0237,  0.0163,  0.0064,  0.0125,\n",
            "         0.0073,  0.0423,  0.0101,  0.0121,  0.0146,  0.0383,  0.0250,  0.0077,\n",
            "         0.0089, -0.0090,  0.0278,  0.0471,  0.0150,  0.0333, -0.0082, -0.0136,\n",
            "        -0.0184,  0.0233,  0.0578, -0.0367, -0.0079,  0.0267, -0.0043,  0.0010,\n",
            "        -0.0068,  0.0390, -0.0037, -0.0251, -0.0292,  0.0319, -0.0021,  0.0374,\n",
            "         0.0490,  0.0510,  0.0263,  0.0310, -0.0241, -0.0245,  0.0098,  0.0439,\n",
            "        -0.0206,  0.0321,  0.0510, -0.0317, -0.0123,  0.0092,  0.0346,  0.0363,\n",
            "        -0.0006,  0.0498, -0.0201, -0.0096,  0.0369,  0.0529,  0.0316, -0.0252,\n",
            "        -0.0339, -0.0176, -0.0271,  0.0136,  0.0289,  0.0110,  0.0070, -0.0025,\n",
            "         0.0238,  0.0384,  0.0046,  0.0270,  0.0044,  0.0445,  0.0430,  0.0391,\n",
            "         0.0267, -0.0393,  0.0275, -0.0357,  0.0385, -0.0034,  0.0441,  0.0474,\n",
            "        -0.0080, -0.0236,  0.0486, -0.0187, -0.0017, -0.0003, -0.0397, -0.0031,\n",
            "         0.0048,  0.0511,  0.0275, -0.0171, -0.0491, -0.0131,  0.0286,  0.0093,\n",
            "        -0.0359,  0.0265,  0.0189,  0.0177, -0.0031,  0.0436,  0.0031,  0.0088,\n",
            "        -0.0315,  0.0168, -0.0072, -0.0280,  0.0427,  0.0431, -0.0134, -0.0474,\n",
            "         0.0089,  0.0453, -0.0239,  0.0101,  0.0184,  0.0436, -0.0158, -0.0320,\n",
            "        -0.0107,  0.0045,  0.0050,  0.0145, -0.0274,  0.0120, -0.0120,  0.0447,\n",
            "         0.0151,  0.0383, -0.0144, -0.0389, -0.0154, -0.0303, -0.0352, -0.0200,\n",
            "         0.0538,  0.0441,  0.0223,  0.0249, -0.0231,  0.0407,  0.0133,  0.0650,\n",
            "        -0.0264,  0.0268,  0.0219, -0.0150, -0.0401,  0.0320, -0.0079,  0.0514,\n",
            "        -0.0065,  0.0073,  0.0354,  0.0211, -0.0347,  0.0208,  0.0460, -0.0259,\n",
            "         0.0009,  0.0216, -0.0232,  0.0473, -0.0215,  0.0462, -0.0167, -0.0281,\n",
            "         0.0474, -0.0323, -0.0142, -0.0069, -0.0238,  0.0491,  0.0329,  0.0395,\n",
            "         0.0166, -0.0105,  0.0521,  0.0179,  0.0529,  0.0068,  0.0060, -0.0148]))\n",
            "('lstm.attended_memory_layernorm.weight', tensor([[0.9714, 0.9993, 0.9483,  ..., 1.0203, 1.0642, 1.0592],\n",
            "        [0.8298, 0.7646, 0.8268,  ..., 0.6350, 0.5864, 0.7118]]))\n",
            "('lstm.attended_memory_layernorm.bias', tensor([[ 0.0072, -0.0052, -0.0014,  ...,  0.0086,  0.0033,  0.0004],\n",
            "        [ 0.0902,  0.0246,  0.0277,  ...,  0.1537,  0.0926,  0.1181]]))\n",
            "('lstm.attended_memory_layernorm2.weight', tensor([[1.1238, 1.2194, 1.1601,  ..., 1.1543, 1.1534, 1.1885],\n",
            "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]))\n",
            "('lstm.attended_memory_layernorm2.bias', tensor([[-0.0140,  0.0466,  0.0420,  ..., -0.0116, -0.0038, -0.0028],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]))\n",
            "('lstm.input_projector.weight', tensor([[ 0.0857,  0.1962, -0.2067,  ...,  0.0232, -0.0282, -0.1374],\n",
            "        [-0.1223,  0.1330,  0.3870,  ..., -0.0224,  0.0381, -0.1482],\n",
            "        [-0.1670, -0.2396, -0.0565,  ...,  0.0633, -0.0788,  0.0881],\n",
            "        ...,\n",
            "        [ 0.0379,  0.0471, -0.3130,  ...,  0.0220,  0.1551,  0.1268],\n",
            "        [-0.0903,  0.1604, -0.1738,  ..., -0.0917, -0.0123,  0.1081],\n",
            "        [ 0.1908,  0.2572,  0.0450,  ..., -0.0864, -0.1149,  0.0298]]))\n",
            "('lstm.input_projector.bias', tensor([-3.5637e-03, -6.0111e-02,  1.1538e-01,  1.4943e-01,  9.4201e-02,\n",
            "        -2.0918e-02,  8.3717e-02, -1.1309e-01, -1.2978e-01, -1.3900e-01,\n",
            "        -6.9580e-02, -5.8632e-03,  1.0211e-01,  1.2048e-01,  5.5348e-02,\n",
            "         1.2056e-01,  1.4131e-01, -2.2794e-02, -3.3363e-02, -1.6284e-02,\n",
            "        -1.0416e-01, -5.8460e-02,  5.3316e-02,  3.2683e-02, -8.8665e-02,\n",
            "         3.3524e-02,  1.5192e-01,  3.1702e-02, -4.4373e-02,  1.1048e-01,\n",
            "         4.2656e-02, -7.7876e-02,  1.2517e-01, -8.6356e-02, -9.3250e-02,\n",
            "         9.0533e-02,  1.1034e-02,  1.4459e-01,  1.1801e-01, -8.1380e-02,\n",
            "        -1.1037e-01, -6.0264e-02,  2.5325e-03,  9.2801e-02,  4.5635e-03,\n",
            "         5.7690e-02, -1.3392e-02,  7.9308e-02,  7.8578e-02,  1.4375e-01,\n",
            "         5.4825e-02, -7.5253e-02,  1.4408e-01, -4.8029e-02, -6.8225e-02,\n",
            "         5.3458e-02,  6.2383e-02, -8.6980e-04,  1.3324e-01,  1.8157e-02,\n",
            "        -2.1057e-02,  1.3399e-02,  3.2681e-02,  5.7641e-02,  7.1570e-02,\n",
            "         6.4555e-02, -4.6835e-02,  1.0995e-01, -5.0568e-02, -2.0297e-02,\n",
            "        -3.3695e-02, -1.4110e-02, -1.7811e-02,  8.9408e-02,  6.2315e-02,\n",
            "         9.3936e-02,  1.4762e-01, -3.4673e-02,  6.7455e-02,  7.1178e-02,\n",
            "         1.2991e-01,  4.2000e-03, -1.3665e-01, -1.4491e-01,  8.1523e-02,\n",
            "        -1.2710e-01, -1.7809e-02,  1.4589e-01, -6.7221e-02, -8.0073e-02,\n",
            "         1.5753e-01, -1.1451e-01, -1.4216e-01,  1.4816e-01,  1.3219e-01,\n",
            "        -4.2885e-03,  1.3073e-01,  5.1649e-02,  1.1866e-01, -8.2004e-02,\n",
            "        -1.4744e-01,  1.4870e-01, -4.0417e-02,  3.2186e-03,  1.3077e-01,\n",
            "         3.5297e-02,  4.6496e-02,  1.4436e-01,  1.9392e-03, -1.2823e-01,\n",
            "         2.8327e-02,  1.1419e-01, -9.1936e-02, -1.2262e-01, -1.4424e-01,\n",
            "        -1.1674e-02,  1.2345e-01, -1.4728e-01, -6.2167e-02,  4.0455e-02,\n",
            "         6.2126e-02, -1.1614e-01,  9.3795e-02, -9.0317e-02,  1.2191e-01,\n",
            "        -5.4797e-02, -6.3729e-02, -1.0315e-01, -5.4000e-02, -6.8629e-03,\n",
            "         8.6283e-04,  1.4118e-01,  6.8153e-03, -2.3758e-02,  1.5078e-01,\n",
            "        -6.5182e-03,  4.7193e-02,  3.6314e-02,  8.1896e-02, -3.5474e-02,\n",
            "         4.5423e-02, -1.6621e-02, -1.3463e-01, -2.0793e-02,  9.7281e-02,\n",
            "        -1.2070e-01, -5.7825e-03, -1.0821e-01, -5.7794e-02, -3.3070e-03,\n",
            "        -8.1966e-02, -9.0403e-02, -6.0397e-02, -8.7483e-02, -1.2713e-01,\n",
            "        -1.1441e-01,  6.7001e-02,  9.2302e-02,  1.5171e-01, -2.1125e-02,\n",
            "        -5.9213e-04,  7.4449e-02, -1.4212e-04, -5.8816e-02, -1.2592e-01,\n",
            "        -1.2519e-01, -6.6236e-02,  3.9420e-03, -1.3638e-01, -1.4630e-01,\n",
            "        -4.9654e-02, -1.2779e-01,  1.2565e-01, -1.0392e-02,  1.0407e-01,\n",
            "         1.2647e-03,  1.1787e-01,  4.9191e-02, -6.9552e-02, -6.2339e-02,\n",
            "         1.5381e-02, -6.6679e-02, -1.3764e-01, -8.0256e-02,  6.5464e-02,\n",
            "         7.4073e-02, -6.0653e-02, -8.5687e-02,  1.5538e-01, -4.0717e-02,\n",
            "         9.6645e-02, -1.4858e-01, -9.7169e-02, -1.4980e-01, -3.6134e-02,\n",
            "         7.3916e-02, -3.6715e-02, -3.9815e-02,  7.8352e-02, -1.6021e-01,\n",
            "         1.2622e-02, -1.0883e-01, -4.4138e-02, -9.0757e-02, -5.7819e-02,\n",
            "        -7.5589e-02,  1.3868e-01, -6.9074e-03, -8.1029e-03, -6.8741e-02,\n",
            "         1.3357e-01, -1.8234e-02,  1.3422e-01,  1.2867e-01,  4.2853e-02,\n",
            "         1.1301e-01, -1.0991e-01,  4.0934e-02,  1.0609e-01,  1.4489e-01,\n",
            "         1.0845e-01,  1.3883e-01, -1.3912e-02, -1.2462e-01,  7.0228e-02,\n",
            "        -9.3750e-02,  1.2291e-01,  9.3187e-02, -8.3849e-02, -3.5237e-02,\n",
            "         5.2553e-02, -3.4336e-02, -2.9061e-02, -1.4581e-01,  2.9157e-02,\n",
            "         5.0351e-02,  1.4767e-02,  1.5038e-01,  1.1655e-01, -5.1423e-02,\n",
            "         1.0508e-01,  6.6289e-02, -1.1684e-01,  3.4663e-02,  7.2564e-02,\n",
            "         4.0013e-02, -1.1028e-01, -4.0178e-02,  5.1098e-02,  1.4939e-01,\n",
            "         6.8873e-02, -9.5499e-03, -1.2389e-01, -2.3904e-03,  1.0345e-01,\n",
            "         3.3488e-02,  1.8116e-02,  1.4780e-01, -9.1123e-02, -7.8499e-02,\n",
            "         5.2920e-02,  1.0482e-01,  1.1514e-01, -5.0531e-02, -2.5727e-02,\n",
            "         5.5087e-02,  6.9941e-02, -8.6738e-03,  8.6091e-02, -6.3900e-02,\n",
            "         8.0056e-02, -8.2417e-02, -2.9911e-02, -1.3509e-01, -1.3089e-01,\n",
            "        -3.8644e-02, -1.0365e-01, -3.9067e-02,  9.0043e-03,  5.2819e-02,\n",
            "         7.7144e-02,  8.4001e-02, -9.1165e-02,  9.9685e-02, -2.7208e-02,\n",
            "         1.1522e-01, -1.4703e-01, -1.1037e-01,  5.0606e-02,  1.3218e-01,\n",
            "        -5.9078e-02, -6.5527e-02,  1.0687e-01,  8.4873e-02, -5.3501e-02,\n",
            "        -7.6501e-02, -5.7584e-02,  1.1182e-02,  3.3156e-02,  8.1420e-02,\n",
            "        -7.3812e-02, -6.6020e-02,  8.6997e-02,  4.6931e-03,  4.5510e-02,\n",
            "         9.5387e-02, -4.0212e-02,  8.0959e-02, -8.3894e-02,  4.5200e-02,\n",
            "         1.1740e-01,  1.4365e-01, -7.6220e-02,  1.4654e-01,  7.4497e-02,\n",
            "         1.7744e-02, -4.8872e-02, -1.3530e-02,  1.1877e-01, -1.1703e-01,\n",
            "        -6.7352e-02,  1.5041e-01,  7.4373e-02,  1.4107e-01, -1.2769e-01,\n",
            "        -3.2709e-02,  4.5144e-02, -9.1224e-02, -4.1388e-02,  1.4082e-01,\n",
            "        -1.1540e-01, -1.4971e-01, -7.1676e-02,  4.5454e-02, -3.8308e-02,\n",
            "         1.0129e-01, -6.9041e-02, -3.7011e-02,  5.7705e-02,  5.7948e-02,\n",
            "        -1.0387e-01,  2.0859e-02, -8.6104e-02, -1.2089e-01,  1.2701e-02,\n",
            "         1.2154e-01, -1.5116e-02,  2.4004e-03, -8.7157e-03, -1.2502e-01,\n",
            "        -2.5756e-03, -2.5571e-02, -4.8820e-02, -1.3621e-01,  9.1896e-02,\n",
            "        -1.2857e-01, -1.3519e-01,  4.0035e-02, -5.3368e-02,  9.8861e-02,\n",
            "         1.4882e-01, -3.0358e-02, -1.2431e-01, -1.3010e-01,  1.1403e-01,\n",
            "        -1.7070e-03, -4.6478e-03,  1.3439e-01,  4.0225e-02,  1.1892e-01,\n",
            "         6.5469e-02, -7.3215e-02,  9.9111e-02,  1.5260e-01,  1.1478e-01,\n",
            "        -7.5647e-02,  1.3981e-01, -8.0626e-02,  1.3466e-01, -1.1860e-01,\n",
            "        -4.4629e-02,  1.2582e-01,  1.0399e-01, -4.6061e-02,  1.3137e-01,\n",
            "        -4.3885e-02,  5.7575e-02,  6.2367e-02,  5.7769e-02, -1.1190e-01,\n",
            "         1.0450e-01, -6.4779e-02, -4.4499e-02,  8.9994e-02,  9.5749e-02,\n",
            "        -1.2080e-01,  1.5166e-01,  2.1996e-02,  3.3632e-02, -1.2263e-01,\n",
            "        -6.0477e-02,  4.3306e-02,  1.3795e-01, -1.3611e-01,  1.2285e-01,\n",
            "         4.7531e-03,  9.4188e-02, -1.1343e-01, -1.0256e-02, -1.3650e-01,\n",
            "        -1.1776e-01, -1.0641e-01, -1.2638e-01, -3.3366e-02, -9.1569e-02,\n",
            "        -4.7409e-02,  5.4135e-02,  8.5945e-02, -7.5186e-02,  1.5651e-02,\n",
            "         1.4229e-01, -1.5406e-01,  5.4105e-03, -1.3315e-01,  1.2350e-01,\n",
            "         1.1656e-01,  2.9320e-02,  4.8716e-02,  8.3114e-02,  2.6219e-02,\n",
            "         3.1666e-02,  1.3298e-01, -3.1166e-02, -1.1829e-01, -1.0133e-01,\n",
            "         1.6715e-02,  3.3240e-02, -1.3308e-01,  1.4101e-01, -9.9194e-02,\n",
            "         9.4843e-02,  6.5140e-02,  3.2894e-03, -3.8433e-02, -5.1623e-02,\n",
            "        -1.1275e-01, -5.2810e-02, -6.0248e-03,  1.1511e-02, -4.5482e-02,\n",
            "         1.0069e-01, -5.0780e-02,  2.9381e-02, -1.7362e-02, -1.2782e-01,\n",
            "         1.1041e-01, -3.4148e-02, -1.1208e-01,  1.5019e-01,  1.4868e-01,\n",
            "         3.0324e-02,  6.2016e-02,  6.0326e-02,  7.8143e-02,  1.4294e-01,\n",
            "         1.5985e-02,  1.3314e-01,  9.1866e-02, -1.4852e-01, -1.1267e-01,\n",
            "         5.6746e-02,  6.6981e-02, -4.7742e-02,  1.0213e-01,  8.8346e-02,\n",
            "         1.5027e-01,  1.3227e-01, -3.3830e-02, -2.4891e-02, -2.9836e-02,\n",
            "         3.3974e-02,  2.5139e-02,  1.2791e-02,  2.9225e-02,  9.8358e-02,\n",
            "         1.2819e-01, -6.9942e-02, -1.2580e-01,  5.4983e-02, -4.2523e-02,\n",
            "         1.3383e-01,  1.3401e-01,  4.3122e-02, -2.2792e-02, -1.8094e-02,\n",
            "         8.7524e-02,  1.1115e-01, -1.1299e-01, -7.2892e-02, -1.4526e-02,\n",
            "        -1.2442e-01, -7.5134e-02, -1.4827e-01, -8.0776e-02, -8.1794e-02,\n",
            "        -1.4643e-01, -1.3994e-01,  6.8147e-02, -3.9542e-02,  6.8904e-02,\n",
            "        -1.0152e-01,  3.9698e-02]))\n",
            "('lstm.input_gate_projector.weight', tensor([[-0.0117, -0.0351,  0.0078,  ...,  0.0130, -0.0197,  0.0208],\n",
            "        [ 0.0309, -0.0395, -0.0521,  ...,  0.0095,  0.1033,  0.0920],\n",
            "        [ 0.0176, -0.0747,  0.0120,  ..., -0.0041,  0.0104,  0.0860],\n",
            "        ...,\n",
            "        [-0.0508,  0.0990, -0.0188,  ..., -0.0177, -0.0866, -0.1173],\n",
            "        [ 0.0322,  0.0367, -0.0272,  ..., -0.0014,  0.0566,  0.0101],\n",
            "        [ 0.0161,  0.0075,  0.0099,  ..., -0.0187, -0.0135,  0.0193]]))\n",
            "('lstm.input_gate_projector.bias', tensor([ 0.0014, -0.0005,  0.0046,  ..., -0.0384,  0.0030,  0.0139]))\n",
            "('lstm.memory_gate_projector.weight', tensor([[ 0.0594, -0.0088,  0.0459,  ...,  0.0424,  0.0274,  0.0053],\n",
            "        [ 0.0093,  0.0383,  0.0454,  ..., -0.0110, -0.0469,  0.0136],\n",
            "        [ 0.0050,  0.0064, -0.0037,  ..., -0.0465,  0.0213,  0.0108],\n",
            "        ...,\n",
            "        [ 0.0070,  0.0051,  0.0543,  ...,  0.0313, -0.0466,  0.0023],\n",
            "        [-0.0570,  0.0024,  0.0060,  ...,  0.0024,  0.0238, -0.0667],\n",
            "        [-0.0592,  0.0644, -0.0026,  ...,  0.0389,  0.0198, -0.0198]]))\n",
            "('lstm.memory_gate_projector.bias', tensor([0.0031, 0.0518, 0.0319,  ..., 0.0247, 0.0269, 0.0422]))\n",
            "('lstm2out.weight', tensor([[ 0.0357,  0.0551,  0.0061,  ...,  0.0216, -0.0190, -0.0589],\n",
            "        [-0.0539, -0.0197, -0.0395,  ...,  0.0422,  0.0069, -0.0002],\n",
            "        [ 0.0378, -0.0229, -0.0126,  ..., -0.0572,  0.0155,  0.0889],\n",
            "        ...,\n",
            "        [-0.0913, -0.0151, -0.0377,  ...,  0.0624,  0.0815,  0.0614],\n",
            "        [ 0.0419, -0.0245,  0.0204,  ...,  0.0422, -0.0498, -0.0316],\n",
            "        [-0.0250, -0.0275, -0.0227,  ..., -0.0106,  0.0724,  0.0383]]))\n",
            "('lstm2out.bias', tensor([-0.0057, -0.0171,  0.0063,  ..., -0.0282, -0.0245, -0.0227]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(gen_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At-TFsMyJ13U",
        "outputId": "b2282b3d-9191-44ff-9d8e-4a9d73144fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "O6H-8E7RJ4JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dadf8e4-8d19-4f26-decf-fc487ff34e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CatGAN_G(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (embeddings): Embedding(3104, 32, padding_idx=0)\n",
              "  (linear): Linear(in_features=32, out_features=3104, bias=True)\n",
              "  (softmax): LogSoftmax(dim=-1)\n",
              "  (lstm): RelationalMemory(\n",
              "    (qkv_projector): Linear(in_features=512, out_features=1536, bias=True)\n",
              "    (qkv_layernorm): LayerNorm((2, 1536), eps=1e-05, elementwise_affine=True)\n",
              "    (attention_mlp): ModuleList(\n",
              "      (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "    (attended_memory_layernorm): LayerNorm((2, 512), eps=1e-05, elementwise_affine=True)\n",
              "    (attended_memory_layernorm2): LayerNorm((2, 512), eps=1e-05, elementwise_affine=True)\n",
              "    (input_projector): Linear(in_features=37, out_features=512, bias=True)\n",
              "    (input_gate_projector): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (memory_gate_projector): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  )\n",
              "  (lstm2out): Linear(in_features=512, out_features=3104, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7qazqbEJ6Xu",
        "outputId": "ad045d19-e941-4a60-b81d-79a92464a475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CatGAN_G(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=32, out_features=32, bias=True)\n",
              "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (embeddings): Embedding(3104, 32, padding_idx=0)\n",
              "  (linear): Linear(in_features=32, out_features=3104, bias=True)\n",
              "  (softmax): LogSoftmax(dim=-1)\n",
              "  (lstm): RelationalMemory(\n",
              "    (qkv_projector): Linear(in_features=512, out_features=1536, bias=True)\n",
              "    (qkv_layernorm): LayerNorm((2, 1536), eps=1e-05, elementwise_affine=True)\n",
              "    (attention_mlp): ModuleList(\n",
              "      (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "    (attended_memory_layernorm): LayerNorm((2, 512), eps=1e-05, elementwise_affine=True)\n",
              "    (attended_memory_layernorm2): LayerNorm((2, 512), eps=1e-05, elementwise_affine=True)\n",
              "    (input_projector): Linear(in_features=37, out_features=512, bias=True)\n",
              "    (input_gate_projector): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (memory_gate_projector): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  )\n",
              "  (lstm2out): Linear(in_features=512, out_features=3104, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "351LtR4oJ9FM",
        "outputId": "d61645a5-6f23-4cd1-a782-1d9ae1f4589a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.text_process import load_dict, tensor_to_tokens"
      ],
      "metadata": {
        "id": "muc3PTctc-Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L1_dict = load_dict('L1')"
      ],
      "metadata": {
        "id": "yEdSuyH1dVgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5):\n",
        "    print(f'---category {i+1}---')\n",
        "    generated = model.sample(10, 32, label_i=i)\n",
        "    sents = tensor_to_tokens(generated, L1_dict[1])\n",
        "    for j in sents:\n",
        "        print(' '.join(j))"
      ],
      "metadata": {
        "id": "w4YDrz8yDamO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    ( adversary )"
      ],
      "metadata": {
        "id": "dXstbwH1DjrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5):\n",
        "    print(f'---category {i+1}---')\n",
        "    generated = model.sample(10, 32, label_i=i)\n",
        "    sents = tensor_to_tokens(generated, L1_dict[1])\n",
        "    for j in sents:\n",
        "        print(' '.join(j))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_ZI4lfIKCs3",
        "outputId": "47f189f7-a329-4dfd-adda-b421fe118da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---category 1---\n",
            "he hands opinion that it of us for the slice .\n",
            "`` for that this , governments awfull is difficult about for women women women 's 's 's - asia % is % asia share\n",
            "`` firstly , are approximately interesting interesting . ''\n",
            "in another government there projected in which which population people whose in age age amount of us than tendency all .\n",
            "the graph demonstrates the several and several qualication females females in countries the countries completed years .\n",
            "they gave often not of health own education our life bible .\n",
            "`` `` in the of one of children people and be go to the biggest 15-59 '' biggest states in yemen\n",
            "`` however , the is is same rise positive same : because between the between and between 9 and 13 going going\n",
            "`` besides , all start about to be wise which will with the dominant sector about of over education in the percentage of\n",
            "all according in all own common one the of common yemen of the period people road and the percentage of children\n",
            "---category 2---\n",
            "`` furthermore , accepting another talentless girl to math almost math , example for example example , because the of stated amount of students\n",
            "`` after , it is clear a started to to in the number of old-age usa the 27 . ''\n",
            "nowadays all things that to to we place about about falling is what is the way to .\n",
            "`` actually , it been will like big searchers for on on a pitches that places understanding ride a . ''\n",
            "in the middle east we can in about the amount of manufactured and ( began unemployed by and has 5 % .\n",
            "`` compared , it children introduce to primary school by and is the one the smallest amount of children , the to\n",
            "they are without all great decision wear in in the of clothing .\n",
            "furthermore is airplanes to that the of building building students building way is , which cases which they working 11 's . and\n",
            "another people increased that air in indonesia indonesia to the amount of air cinema that students the of students was .\n",
            "`` the disease us % of this tendency people , sexes over different like % to . ''\n",
            "---category 3---\n",
            "in 2000 is total auditory auditory idea which to not for the number of male and students decrets ?\n",
            "`` first , women a , many 1 can and be develop math decrease in modern , means , means improve\n",
            "`` according to the statistic charts , 2000 on in , 61,6 increase a of economics work a lot of\n",
            "`` the linkedin metro can an auditory auditory is is repeated . ''\n",
            "`` degree things it , that of plane children pollution and have have pollution have by in as and every to big boat\n",
            "`` for in contrast , instance , the of history are one , the of usa addition company 10 in comes comes the same of\n",
            "`` to sum , some classmate may important to a actual actual important of among . ''\n",
            "`` overview a closer '' see of , it : of date ; date a : of significantly by will . ''\n",
            "there goverment of for such as such and as as for this as for for as life for this life life .\n",
            "`` according to to , the of young age spend of 1 is , to not their decrease . ''\n",
            "---category 4---\n",
            "they begin fossil with on and come germany % .\n",
            "`` however , all been showing with the results results of the main difference and a main trend , these\n",
            "`` the number of is 15 who to in quite to 5 millions 5 millions in yemen and consist\n",
            "`` in the in usa women the uk the group of who was money to 2012 to , at are more\n",
            "12 can for third regions were those : to the were by rail .\n",
            "in most millions about are level people for in asia are .\n",
            "the graph below how on about the task amount of male and female who after after after the school\n",
            "in japan there minerals so fall and the 1998 from both people to ( people the people .\n",
            "the bar chart below information about depicts depicts the depicts depicts women that obesity about obesity from 1985 12 from during\n",
            "`` in the middle east we number of 2000 was % of was fall get in 2015 2015 % in\n",
            "---category 5---\n",
            "the missiles budget is $ millions each .\n",
            "`` in to , it amount of building of was without ability the problem for to the interests , it the of the\n",
            "`` in the second whole of , its it different without daughters for in different countries have usually move to usually\n",
            "the minimum bid total is $ 3 dollars .\n",
            "`` others for , the general is cause we , children in by can students by , we an rate of ? ''\n",
            "`` the world world , it of its can ones have to in the tendency the of uk angeles , the of\n",
            "`` the is all of ebooks the age people in people aged , agricultural in 's uk is four 's usa . ''\n",
            "`` moreover , no they good would every work . ''\n",
            "authors this year to approximately from the period drop to in the period period period millions middle .\n",
            "this contract toll worth $ 72 millions .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  80   adversarial :"
      ],
      "metadata": {
        "id": "QOHje1ouQyAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5):\n",
        "    print(f'---category {i+1}---')\n",
        "    generated = model.sample(10, 32, label_i=i)\n",
        "    sents = tensor_to_tokens(generated, L1_dict[1])\n",
        "    for j in sents:\n",
        "        print(' '.join(j))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZxpMS4rQxpB",
        "outputId": "fbd37b6f-7f62-451f-992d-812999f6e222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---category 1---\n",
            "`` all in yemen , there of obviously needs pirate only break are for the dealers and steal big money to it\n",
            "they is is say to another education another or another or way to realise .\n",
            "the hospital this employed indigenous indigenous staff in this amount doctors .\n",
            "`` facebook , i , would be most in groups and network , which all want is to four four age way in\n",
            "after on the percentage there level of facebook grow up to up be number of people to be .\n",
            "a detailed people it is people the pie for the percentage that of it to fluctuated it more than than\n",
            "`` at , in the percentage of the maximum was the point of was will - was - the same of\n",
            "yet gave of the has will students and and the age low was years will 24.1 than than the amount of .\n",
            "`` to to sum up it , there people to a faculty faculty of reducing air pollution a life people and to ,\n",
            "`` according different half of had was to be , that should the extremely extremely england is their extremely in then protect ,\n",
            "---category 2---\n",
            "the bar-charts challenge can the numbers of their by women different ages .\n",
            "they is them that it classes the travelling and the of they want to .\n",
            "`` in as the lowest world point also and that which be ( the amount of children children access to 11\n",
            "all amount went off off to 33 millions .\n",
            "`` firstly from it , is that get make to different time say on 16 meetings dramatically in 16 , . '' that\n",
            "`` of my people pollution , i to influence to find , for their for example bad for air to that .\n",
            "`` all things in , i in the quantity of investment children ability to more a primary school and in violence\n",
            "the amount of over can over a states took to slightly parts of under the quarter of all all .\n",
            "`` taking ships not not , i to will higher in on air warming , but their their of bike and\n",
            "`` the underground in both amount of goods by people , connect to the year the by amount of desctop there violence\n",
            "---category 3---\n",
            "air situation for have to the and hi-teck .\n",
            "a lot of note and that hi-teck implies costs young than the .\n",
            "industry can these how on the fall hi-teck reduce what .\n",
            "`` the problem is information about information very is information information , which can be to and primary in changed children in . ''\n",
            "`` in addition countries , have weight to a and number of modern people people people , while modern and\n",
            "in hi-teck young are the there figures for actual for .\n",
            "he by business hi-teck founded all exactly .\n",
            "`` as the temp hand very hand companies spend not not of percents : which suit is girls per money , of live\n",
            "`` all in all , all facebook the period is in the most across countries age in age , usa instagram for but\n",
            "nowadays many are a connect problems actual that as ecological technology .\n",
            "---category 4---\n",
            "it of be 2015 less seen millions and by people by people of not in this become difference in by .\n",
            "`` according 2012 , it 1995 to to primary see the income in the of dominant education is from the same\n",
            "`` in comparising , 2013 11 popular popular , only of women , between and it almost it overweight around it of it it\n",
            "`` overall , the chart is the data in number of who both do n't on performances have performances and has has\n",
            "`` overall , it analysing charts the popular mobile among , facebook facebook facebook , and 2012 the 2013 fall and 2012 to\n",
            "`` in 2012 , year in south there s. asia is millions not of access the to millions for . '' ''\n",
            "`` at , the percentage of who childrens to on sent a school is in the in 2008 of unemployment .\n",
            "`` due the year from the group the most from to the countries , are in 1999 and the sector in the\n",
            "`` there in there of in asia no millions in 2014 2014 and asia millions worldwide . ''\n",
            "`` for to other , example from railway the has system in the usa group in point which in to the changed\n",
            "---category 5---\n",
            "in 200809 were people is $ billions millions .\n",
            "`` all in , apple comes there two sectors that there the tendency tendency tendency to , the fall problem for to the\n",
            "it is approximately argued to $ not with .\n",
            "the freight congressional totals has 49 49 49 25 1 for .\n",
            "`` it of worth take clear on print from from , the general , is for as a as or teacher . ''\n",
            "`` othes who would equall and air and have a poverty '' .\n",
            "`` according to the chart the most , changing is was for in asia , in 16.8 millions girl got got access to\n",
            "all it is dollar should female in singers singers .\n",
            "`` after years , there in and was sharp very very number of people people people people people people % of people was % people . ''\n",
            "the total expenses eventually eventually $ billions billions .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  220   adversarial :"
      ],
      "metadata": {
        "id": "lcQzoVdTc8QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (5):\n",
        "    print(f'---category {i+1}---')\n",
        "    generated = model.sample(10, 32, label_i=i)\n",
        "    sents = tensor_to_tokens(generated, L1_dict[1])\n",
        "    for j in sents:\n",
        "        print(' '.join(j))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHiIXgWkCe1F",
        "outputId": "d54377dc-526f-454a-c5dc-c2ab5aaadc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---category 1---\n",
            "`` albeit generally france , the line in males had girls equal , but and it in i in respectively % in the\n",
            "every had of us has is often .\n",
            "`` other should there ( are are old to and only university in only love , on them , i for , only\n",
            "`` population good who are aged in and this using half of 65 years , but and this middle amount of\n",
            "`` for example , a of fall a ill heart . ''\n",
            "`` people is see friends in 18 , 15 not kind of their age , of daughter to this , all\n",
            "young in their on education year were in people and population 65 years % not over not be % of incregse amount .\n",
            "`` there in in the biggest group in this group group age in age group group , 18 from only group amount of to\n",
            "inside every of a are problems talents .\n",
            "`` any in the graph number of this was reduce was about was in the and group the group of the world amount\n",
            "---category 2---\n",
            "`` in conclusion , we 80th us agree that but all is the amount of girls countries be popular , in flights we\n",
            "`` every parent that it supported has the amount of has has for this for this job is , this reason reason\n",
            "`` in order one , africa is to vere the lowest persentage large of large large large of 3 , to per per of\n",
            "on the first other can indicate two see way about facebook about two have about ) 2013 in in .\n",
            "after almost years the stable dramatic remained the remained dramatic to years years 2013 stable .\n",
            "`` according to the graph first from the amount of in boys , who girls 24 to counted more about primary\n",
            "the charts below population in in london in their is had was in was and was the percentage proportion of\n",
            "a little of it using worth understand seeing and understand make the was and was .\n",
            "`` according to the chart in the , the beginning of people boys , who are significantly increased to the primary school\n",
            "`` men been , however say that a can for , could to a for pollution , this other and . ''\n",
            "---category 3---\n",
            "`` but have a age , it of for children and have more primary education , which are may places ,\n",
            "the whole management the problems are hi-teck and .\n",
            "his initiatives is hi-teck startups consulting companies exactly and .\n",
            "`` percent of there some , are are can not in is can times university times from to per per\n",
            "while can to medicine how more hi-teck century ? -\n",
            "`` a drawback of people auditory that is which that which have that it is be be knowledge of that air like to . ''\n",
            "it is be by investment of and by the world and child a parallel .\n",
            "let has it very latest 's and hi-teck 's .\n",
            "it has is impossible than on by a game by and the .\n",
            "`` according there to to population '' '' population `` '' '' '' people charts in , the usa usa and whatever that in live\n",
            "---category 4---\n",
            "`` as from the regions pensioners and africa , unemployment be years have , who 12,5 % it and they in it asia . ''\n",
            "`` the of data was then have in number of earning boys , who only are grown moved on with this\n",
            "`` the given union states women in 7,5 in south africa , however about a fall started to education at the\n",
            "`` secondly the chart , in the number of transported girls very very 2014 and boys are boys than different in\n",
            "`` futhermore up , china these europe and up where up to up in . ''\n",
            "`` in africa the numbers level of was was high % was 2014 high , while in 140 of has percents percents\n",
            "`` the tendency shows in can in the percentage of all africa , was then have was in by and was\n",
            "`` in comperison the usa in the group and the people people money on on , in the usa 40 % in\n",
            "in addition 2013 more uses down in and down in used .\n",
            "they win on three games in 2014 with 2014 2012 and their in their .\n",
            "---category 5---\n",
            "`` 12 , are considerable good that on the grave damage from , whereas with and the 1970th bottom of years\n",
            "`` moreover south considered the amount of are children in only in asia and 1970th '' . ''\n",
            "`` after , the than key number of was was in was for was % in was was , the number of\n",
            "`` after a , think it is to in the proportion was , and the inereasing to growing growing percents % . ''\n",
            "`` final was were or worth games , is is a tendency to other billions . ''\n",
            "the estimated cost is value 3 billions .\n",
            "`` the illustrates people rural increased the most in about the most most period in , when we more development people use the most of\n",
            "`` it went millions business noticing from to on , the tendency not your , as can suffering for the problem\n",
            "all multi of dollar selling female singers .\n",
            "`` according this , `` see the in in the highest was , there is by by the extremely eu is much . ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DucfArM8Nkxo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}